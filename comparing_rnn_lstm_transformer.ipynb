{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNv9X+8XvRBVV7mZ/oqZdyQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ek0212/comparing-rnn-lstm-transformer/blob/main/comparing_rnn_lstm_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Required Libraries ---\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"--- Welcome to the Neural Network Sequence Processor Explorer! ---\")\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"\\nWe're going to see how different types of neural networks 'read' and 'remember' a simple sequence: 'a b c'.\")\n",
        "print(\"We'll explore:\")\n",
        "print(\"1. RNN (Recurrent Neural Network): Reads step-by-step, basic memory.\")\n",
        "print(\"2. LSTM (Long Short-Term Memory): Smarter step-by-step reading, better memory with 'gates'.\")\n",
        "print(\"3. Transformer (Self-Attention): Looks at the whole sequence at once to see how parts relate.\")\n",
        "\n",
        "print(\"\\n--- Setting up our 'Ingredients' ---\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "print(\"\\n[SETUP] Setting a 'random seed' (42). This ensures that if we run this code again,\")\n",
        "print(\"        any 'random' numbers generated (like initial weights) will be the exact same.\")\n",
        "print(\"        It's like starting a card game with the deck shuffled the same way every time for an experiment.\")\n",
        "\n",
        "# Vocabulary: mapping characters to integers\n",
        "vocab = {'a': 0, 'b': 1, 'c': 2}\n",
        "vocab_inv = {v: k for k, v in vocab.items()}\n",
        "print(\"\\n[SETUP] Creating a 'vocabulary'. Computers like numbers, not letters directly.\")\n",
        "print(f\"        Our vocab: {vocab} (maps letters to numbers)\")\n",
        "print(f\"        And an inverse vocab: {vocab_inv} (maps numbers back to letters)\")\n",
        "\n",
        "input_sequence = torch.tensor([0, 1, 2])  # This represents the sequence \"a b c\"\n",
        "print(\"\\n[SETUP] Our input sequence is 'a b c', represented by numbers:\", input_sequence.tolist())\n",
        "print(\"        So, 'a' is 0, 'b' is 1, 'c' is 2.\")\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_size = 2\n",
        "hidden_size = 2\n",
        "print(f\"\\n[SETUP] Vocab size: {vocab_size} (we have 3 unique characters: a, b, c)\")\n",
        "print(f\"[SETUP] Embedding size: {embedding_size}. Each character will be represented by a list (vector) of {embedding_size} numbers.\")\n",
        "print(\"        Think of these as 'features' or 'traits' for each character.\")\n",
        "print(f\"[SETUP] Hidden size: {hidden_size}. This is like the 'memory capacity' or 'thinking space' for our RNN/LSTM.\")\n",
        "\n",
        "# Original prints for input sequence details\n",
        "print(\"\\n===== Toy Input Sequence (Recap) =====\")\n",
        "print(\"Input indices:\", input_sequence.tolist())\n",
        "print(\"Input tokens:\", [vocab_inv[i.item()] for i in input_sequence])\n",
        "\n",
        "# Create random embeddings for each token\n",
        "embedding_matrix = torch.randn(vocab_size, embedding_size)\n",
        "print(\"\\n===== Embedding Matrix (Randomly Initialized) =====\")\n",
        "print(\"[SETUP] Creating an 'Embedding Matrix'. This is a lookup table.\")\n",
        "print(\"        Row 0 = vector for 'a', Row 1 = vector for 'b', Row 2 = vector for 'c'.\")\n",
        "print(\"        In real life, these vectors are *learned*. Here, they are just random numbers to start.\")\n",
        "print(embedding_matrix)\n",
        "print(f\"        For example, the random vector for 'a' (index 0) is: {embedding_matrix[0]}\")\n",
        "\n",
        "### --- RNN (Recurrent Neural Network) --- ###\n",
        "print(\"\\n\\n===============================================\")\n",
        "print(\"===== RNN (Recurrent Neural Network) =====\")\n",
        "print(\"===============================================\")\n",
        "print(\"\\n[RNN INFO] RNNs process sequences one item at a time, like reading a sentence word by word.\")\n",
        "print(\"           They have a 'hidden state' (memory) that tries to capture what's been seen so far.\")\n",
        "print(\"           Let's see how it processes 'a b c'.\")\n",
        "\n",
        "# Define weights and bias for RNN\n",
        "input_to_hidden_weights = torch.randn(embedding_size, hidden_size)\n",
        "hidden_to_hidden_weights = torch.randn(hidden_size, hidden_size)\n",
        "hidden_bias = torch.zeros(hidden_size)\n",
        "print(\"\\n[RNN SETUP] Initializing RNN 'weights' and 'bias'.\")\n",
        "print(\"            These are numbers the RNN would learn if we were training it.\")\n",
        "print(\"            - 'input_to_hidden_weights': Transform the current character's vector.\")\n",
        "print(\"            - 'hidden_to_hidden_weights': Transform the previous memory.\")\n",
        "print(\"            - 'hidden_bias': A small adjustment.\")\n",
        "\n",
        "# Initialize hidden state (memory)\n",
        "hidden_state = torch.zeros(hidden_size)\n",
        "print(\"\\n[RNN SETUP] Initializing the RNN's 'hidden state' (memory) to all zeros.\")\n",
        "print(\"            Before seeing any input, the memory is blank:\", hidden_state)\n",
        "\n",
        "# Loop through each input token\n",
        "for step in range(len(input_sequence)):\n",
        "    token_index = input_sequence[step]\n",
        "    token_char = vocab_inv[token_index.item()]\n",
        "    token_vector = embedding_matrix[token_index]  # Get embedding vector\n",
        "\n",
        "    print(f\"\\n--- RNN Step {step+1}: Processing Token '{token_char}' ---\")\n",
        "    print(f\"Input vector (embedding) for '{token_char}':\", token_vector)\n",
        "    print(\"Hidden state (memory from PREVIOUS step):\", hidden_state)\n",
        "\n",
        "    print(\"\\n[RNN CALC] Updating hidden state using the formula:\")\n",
        "    print(\"           new_hidden = tanh( (current_input_vector @ W_input) + (previous_hidden_state @ W_hidden) + bias )\")\n",
        "    # Update hidden state\n",
        "    input_contribution = token_vector @ input_to_hidden_weights\n",
        "    previous_memory_contribution = hidden_state @ hidden_to_hidden_weights\n",
        "    combined_info = input_contribution + previous_memory_contribution + hidden_bias\n",
        "    hidden_state = torch.tanh(combined_info)\n",
        "    print(f\"Updated RNN hidden state after '{token_char}':\", hidden_state)\n",
        "    print(\"           The 'tanh' function squashes the result between -1 and 1, keeping numbers manageable.\")\n",
        "\n",
        "print(\"\\n[RNN RESULT] Final RNN hidden state after processing 'a b c':\", hidden_state)\n",
        "print(\"             This vector is the RNN's attempt to summarize the entire sequence 'a b c'.\")\n",
        "print(\"             A challenge for RNNs: they can struggle to remember things from far back in long sequences.\")\n",
        "\n",
        "### --- LSTM (Long Short-Term Memory) --- ###\n",
        "print(\"\\n\\n=====================================================\")\n",
        "print(\"===== LSTM (Long Short-Term Memory) =====\")\n",
        "print(\"=====================================================\")\n",
        "print(\"\\n[LSTM INFO] LSTMs are a type of RNN, but with a more sophisticated memory system.\")\n",
        "print(\"            They use 'gates' to control the flow of information, helping them remember things for longer.\")\n",
        "print(\"            The key components are:\")\n",
        "print(\"            - A 'cell state' (C_t): The long-term memory conveyor belt.\")\n",
        "print(\"            - Forget Gate (f_t): Decides what old information to discard from C_t.\")\n",
        "print(\"            - Input Gate (i_t): Decides what new information to store in C_t.\")\n",
        "print(\"            - Output Gate (o_t): Decides what part of C_t to output as the current hidden state (h_t).\")\n",
        "print(\"\\n[LSTM INFO] IMPORTANT: In this demo, the LSTM's weights are random, so it doesn't 'know' what to forget yet.\")\n",
        "print(\"             It 'learns' what's important to forget/remember during a separate 'training' phase, not shown here.\")\n",
        "print(\"             We're just seeing the *mechanics* of how the gates operate with some random weights.\")\n",
        "\n",
        "\n",
        "# LSTM uses gates and memory cell to track long-term patterns\n",
        "lstm_weights = torch.randn(embedding_size + hidden_size, 4 * hidden_size) # 4 for: input, forget, cell_candidate, output gates\n",
        "lstm_bias = torch.zeros(4 * hidden_size)\n",
        "print(\"\\n[LSTM SETUP] Initializing LSTM 'weights' and 'bias'. LSTMs have more weights because of their gates.\")\n",
        "print(\"             These weights combine the current input and previous hidden state to control 4 parts.\")\n",
        "\n",
        "# Initialize LSTM hidden state and cell state\n",
        "hidden_state = torch.zeros(hidden_size) # This is h_{t-1} at the start of each step\n",
        "cell_state = torch.zeros(hidden_size)   # This is C_{t-1} at the start of each step\n",
        "print(\"\\n[LSTM SETUP] Initializing LSTM 'hidden state' (h_t, short-term output) and 'cell state' (C_t, long-term memory) to zeros.\")\n",
        "print(\"            Initial hidden state (h_0):\", hidden_state)\n",
        "print(\"            Initial cell state (C_0):\", cell_state)\n",
        "\n",
        "for step in range(len(input_sequence)):\n",
        "    token_index = input_sequence[step]\n",
        "    token_char = vocab_inv[token_index.item()]\n",
        "    token_vector = embedding_matrix[token_index]\n",
        "\n",
        "    print(f\"\\n--- LSTM Step {step+1}: Processing Token '{token_char}' ---\")\n",
        "    print(f\"Input vector (embedding for '{token_char}'):\", token_vector)\n",
        "    print(f\"Hidden state from PREVIOUS step (h_{{{step}}}):\", hidden_state)\n",
        "    print(f\"Cell state (long-term memory) from PREVIOUS step (C_{{{step}}}):\", cell_state)\n",
        "\n",
        "    print(\"\\n[LSTM CALC] Combining current input vector and previous hidden state (h_t-1).\")\n",
        "    combined_vector = torch.cat([token_vector, hidden_state], dim=0)\n",
        "\n",
        "    print(\"[LSTM CALC] Calculating values for all 4 gates/parts using the combined vector and LSTM weights.\")\n",
        "    gate_values = combined_vector @ lstm_weights + lstm_bias\n",
        "    # The order is often Input, Forget, Cell Candidate (or Gate for cell), Output\n",
        "    # but here we'll follow the original code's split for consistency in demonstration:\n",
        "    input_gate_raw, forget_gate_raw, cell_candidate_raw, output_gate_raw = gate_values.chunk(4, dim=0)\n",
        "\n",
        "    print(\"\\n[LSTM CALC] Applying activation functions to gate values:\")\n",
        "    print(\"            - 'sigmoid' (outputs 0 to 1) is used for gates. This is key!\")\n",
        "    print(\"              A value near 0 means 'close the gate / forget / ignore'.\")\n",
        "    print(\"              A value near 1 means 'open the gate / remember / allow'.\")\n",
        "    print(\"            - 'tanh' (outputs -1 to 1) is used for new candidate values to add to memory.\")\n",
        "\n",
        "    # --- Forget Gate (f_t) ---\n",
        "    # Decides what to forget from the old cell state.\n",
        "    forget_gate = torch.sigmoid(forget_gate_raw)\n",
        "    print(\"\\nForget Gate (f_t) values:\", forget_gate)\n",
        "    print(\"           Interpretation: Each number (0 to 1) decides how much of the corresponding\")\n",
        "    print(f\"           part of the *old* cell state (C_{{{step}}}) to keep. 0 = forget, 1 = keep.\")\n",
        "\n",
        "    # --- Input Gate (i_t) & Cell Candidate (g_t) ---\n",
        "    # Input Gate: Decides which new values to update in the cell state.\n",
        "    input_gate = torch.sigmoid(input_gate_raw)\n",
        "    # Cell Candidate: Creates a vector of new candidate values that *could* be added.\n",
        "    cell_candidate = torch.tanh(cell_candidate_raw)\n",
        "    print(\"\\nInput Gate (i_t) values:\", input_gate)\n",
        "    print(\"           Interpretation: Decides how much of the 'new candidate info' (below) to add to memory.\")\n",
        "    print(\"Cell Candidate (g_t) values (new potential info):\", cell_candidate)\n",
        "    print(\"           Interpretation: These are new values that *could* be added to the cell state.\")\n",
        "\n",
        "    # --- Output Gate (o_t) ---\n",
        "    # Decides what part of the (updated) cell state to output as the hidden state.\n",
        "    output_gate = torch.sigmoid(output_gate_raw)\n",
        "    print(\"\\nOutput Gate (o_t) values:\", output_gate)\n",
        "    print(\"           Interpretation: Decides how much of the *newly updated* cell state (C_t) to output as the hidden state (h_t).\")\n",
        "\n",
        "    print(\"\\n[LSTM CALC] Updating the cell state (C_t) - the LSTM's long-term memory:\")\n",
        "    print(f\"           Formula: C_{{{step+1}}} = (Forget_Gate * C_{{{step}}}) + (Input_Gate * Cell_Candidate)\")\n",
        "    print(f\"           Breaking it down:\")\n",
        "    forgotten_part = forget_gate * cell_state\n",
        "    print(f\"             1. `Forget_Gate * C_{{{step}}}`: {forgotten_part} (Old memory filtered by forget gate)\")\n",
        "    new_info_part = input_gate * cell_candidate\n",
        "    print(f\"             2. `Input_Gate * Cell_Candidate`: {new_info_part} (New candidate info filtered by input gate)\")\n",
        "    cell_state = forgotten_part + new_info_part\n",
        "    print(f\"Updated cell state (C_{{{step+1}}}):\", cell_state)\n",
        "    print(f\"           This new cell state C_{{{step+1}}} now holds a combination of filtered old memories and filtered new information.\")\n",
        "\n",
        "    print(\"\\n[LSTM CALC] Updating the hidden state (h_t) - the output for this step:\")\n",
        "    print(f\"           Formula: h_{{{step+1}}} = Output_Gate * tanh(C_{{{step+1}}})\")\n",
        "    hidden_state = output_gate * torch.tanh(cell_state)\n",
        "    print(f\"Updated hidden state (h_{{{step+1}}}):\", hidden_state)\n",
        "    print(f\"           The hidden state h_{{{step+1}}} is a filtered version of the LSTM's internal long-term memory (C_{{{step+1}}}).\")\n",
        "\n",
        "\n",
        "print(\"\\n[LSTM RESULT] Final LSTM hidden state (h_N) after processing 'a b c':\", hidden_state)\n",
        "print(\"[LSTM RESULT] Final LSTM cell state (C_N) after processing 'a b c':\", cell_state)\n",
        "print(\"              LSTMs, through their gates (especially the forget gate), can learn what information\")\n",
        "print(\"              to retain or discard over long sequences. This 'learning' happens during training,\")\n",
        "print(\"              where weights are adjusted to minimize errors on a specific task.\")\n",
        "\n",
        "### --- Transformer Self-Attention --- ###\n",
        "print(\"\\n\\n===============================================\")\n",
        "print(\"===== Transformer Self-Attention =====\")\n",
        "print(\"===============================================\")\n",
        "print(\"\\n[TRANSFORMER INFO] Transformers (specifically, their 'self-attention' mechanism) work differently.\")\n",
        "print(\"                   Instead of step-by-step, they look at ALL tokens in the sequence AT ONCE.\")\n",
        "print(\"                   Self-attention helps each token figure out how relevant other tokens are to it.\")\n",
        "print(\"                   Analogy: When reading, you might glance at other words in a sentence to understand a specific word's context.\")\n",
        "\n",
        "# Self-attention compares every token to every other token\n",
        "input_vectors = embedding_matrix[input_sequence]  # shape: (3, 2)\n",
        "print(\"\\n[TRANSFORMER SETUP] First, get the embedding vectors for our entire input sequence 'a b c' at once.\")\n",
        "print(\"Input vectors (embeddings for 'a', 'b', 'c'):\\n\", input_vectors)\n",
        "\n",
        "# Create weights for queries, keys, and values\n",
        "W_query = torch.randn(embedding_size, embedding_size)\n",
        "W_key = torch.randn(embedding_size, embedding_size)\n",
        "W_value = torch.randn(embedding_size, embedding_size)\n",
        "print(\"\\n[TRANSFORMER SETUP] For each input token, we create three special versions using learned weights:\")\n",
        "print(\"                   1. Query (Q): What is this token 'looking for' in other tokens?\")\n",
        "print(\"                   2. Key (K): What kind of information does this token 'represent' or 'offer'?\")\n",
        "print(\"                   3. Value (V): What actual information does this token 'contribute' if it's deemed relevant?\")\n",
        "\n",
        "queries = input_vectors @ W_query\n",
        "keys = input_vectors @ W_key\n",
        "values = input_vectors @ W_value\n",
        "\n",
        "print(\"\\n[TRANSFORMER CALC] Generating Query, Key, and Value vectors for each token:\")\n",
        "print(\"Query vectors (Q) - one row per token ('a','b','c'):\\n\", queries)\n",
        "print(\"Key vectors (K) - one row per token:\\n\", keys)\n",
        "print(\"Value vectors (V) - one row per token:\\n\", values)\n",
        "\n",
        "print(\"\\n[TRANSFORMER CALC] Calculating 'Attention Scores':\")\n",
        "print(\"                   How much should token_i 'pay attention' to token_j?\")\n",
        "print(\"                   This is done by: (Query_i @ Key_j.transposed) / sqrt(dimension_of_key)\")\n",
        "attention_scores = queries @ keys.T / np.sqrt(embedding_size)\n",
        "print(\"Attention scores (raw, after scaling):\\n\", attention_scores)\n",
        "print(\"Interpretation: attention_scores[row_i, col_j] = score of token_i attending to token_j.\")\n",
        "print(\"e.g., scores[0,1] is 'a' attending to 'b'. Scaling helps stabilize numbers.\")\n",
        "\n",
        "print(\"\\n[TRANSFORMER CALC] Converting scores to 'Attention Weights' (probabilities) using Softmax.\")\n",
        "print(\"                   Softmax makes sure that for each token, its attention to all other tokens sums up to 1 (like percentages).\")\n",
        "attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "print(\"Attention weights (after softmax):\\n\", attention_weights)\n",
        "print(\"Interpretation: weights[row_i, col_j] = how much % token_i focuses on token_j.\")\n",
        "\n",
        "print(\"\\n[TRANSFORMER CALC] Creating new, 'context-aware' representations for each token.\")\n",
        "print(\"                   Each token's new vector = weighted sum of ALL Value vectors.\")\n",
        "print(\"                   The weights are the attention_weights we just calculated.\")\n",
        "attention_output = attention_weights @ values\n",
        "print(\"\\nAttention output vectors (new representations for 'a','b','c' based on context):\\n\", attention_output)\n",
        "print(\"Each row is a new vector for 'a', 'b', or 'c', now enriched with context from other tokens it 'attended' to.\")\n",
        "print(\"Unlike RNN/LSTM's single final state, Attention gives an updated representation for *each* token.\")\n",
        "\n",
        "\n",
        "### --- Summary --- ###\n",
        "print(\"\\n\\n==========================\")\n",
        "print(\"===== Quick Summary =====\")\n",
        "print(\"==========================\")\n",
        "\n",
        "print(\"\\n--- RNN (Recurrent Neural Network) ---\")\n",
        "print(\"- Reads input one piece at a time (sequential).\")\n",
        "print(\"- Uses a 'hidden state' as its memory, updated at each step.\")\n",
        "print(\"- Analogy: Reading a book word-by-word, trying to remember the plot.\")\n",
        "print(\"- Challenge: Can struggle with long-term memory (forgetting early parts of long sequences).\")\n",
        "\n",
        "print(\"\\n--- LSTM (Long Short-Term Memory) ---\")\n",
        "print(\"- Also sequential, but with a smarter memory system using 'gates' (input, forget, output).\")\n",
        "print(\"- Has a 'cell state' for better long-term memory retention. The 'forget gate' specifically learns\")\n",
        "print(\"  what old information in the cell state is no longer relevant and should be discarded.\")\n",
        "print(\"- Analogy: Reading with a good note-taking system – deciding what to write down, what to erase (forget gate's job!), and what to refer to.\")\n",
        "print(\"- Advantage: Better at handling long sequences and remembering important details over time than basic RNNs because it can learn to manage its memory.\")\n",
        "\n",
        "print(\"\\n--- Transformer (Self-Attention) ---\")\n",
        "print(\"- Looks at ALL input tokens at once (parallel processing).\")\n",
        "print(\"- 'Self-attention' lets each token weigh the importance of all other tokens (including itself) to understand its context.\")\n",
        "print(\"- Uses Query, Key, Value mechanism to calculate these attention weights.\")\n",
        "print(\"- Analogy: Looking at an entire picture at once, and for each object, seeing how it relates to all other objects in the scene.\")\n",
        "print(\"- Advantages: Often better for long sequences, can capture complex relationships, good for parallel computation (training faster).\")\n",
        "\n",
        "print(\"\\n\\n----------------------------------------------------------------------\")\n",
        "print(\"--- End of Demo! ---\")\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"This was a tiny peek. Real models are much bigger and are 'trained' on lots of data to learn their weights,\")\n",
        "print(\"including how the LSTM's forget gate should behave for specific tasks!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyK_jDqw5SrH",
        "outputId": "8781d0b7-f5f5-49dc-fa85-8f3b571dad30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "--- Welcome to the Neural Network Sequence Processor Explorer! ---\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "We're going to see how different types of neural networks 'read' and 'remember' a simple sequence: 'a b c'.\n",
            "We'll explore:\n",
            "1. RNN (Recurrent Neural Network): Reads step-by-step, basic memory.\n",
            "2. LSTM (Long Short-Term Memory): Smarter step-by-step reading, better memory with 'gates'.\n",
            "3. Transformer (Self-Attention): Looks at the whole sequence at once to see how parts relate.\n",
            "\n",
            "--- Setting up our 'Ingredients' ---\n",
            "\n",
            "[SETUP] Setting a 'random seed' (42). This ensures that if we run this code again,\n",
            "        any 'random' numbers generated (like initial weights) will be the exact same.\n",
            "        It's like starting a card game with the deck shuffled the same way every time for an experiment.\n",
            "\n",
            "[SETUP] Creating a 'vocabulary'. Computers like numbers, not letters directly.\n",
            "        Our vocab: {'a': 0, 'b': 1, 'c': 2} (maps letters to numbers)\n",
            "        And an inverse vocab: {0: 'a', 1: 'b', 2: 'c'} (maps numbers back to letters)\n",
            "\n",
            "[SETUP] Our input sequence is 'a b c', represented by numbers: [0, 1, 2]\n",
            "        So, 'a' is 0, 'b' is 1, 'c' is 2.\n",
            "\n",
            "[SETUP] Vocab size: 3 (we have 3 unique characters: a, b, c)\n",
            "[SETUP] Embedding size: 2. Each character will be represented by a list (vector) of 2 numbers.\n",
            "        Think of these as 'features' or 'traits' for each character.\n",
            "[SETUP] Hidden size: 2. This is like the 'memory capacity' or 'thinking space' for our RNN/LSTM.\n",
            "\n",
            "===== Toy Input Sequence (Recap) =====\n",
            "Input indices: [0, 1, 2]\n",
            "Input tokens: ['a', 'b', 'c']\n",
            "\n",
            "===== Embedding Matrix (Randomly Initialized) =====\n",
            "[SETUP] Creating an 'Embedding Matrix'. This is a lookup table.\n",
            "        Row 0 = vector for 'a', Row 1 = vector for 'b', Row 2 = vector for 'c'.\n",
            "        In real life, these vectors are *learned*. Here, they are just random numbers to start.\n",
            "tensor([[ 0.3367,  0.1288],\n",
            "        [ 0.2345,  0.2303],\n",
            "        [-1.1229, -0.1863]])\n",
            "        For example, the random vector for 'a' (index 0) is: tensor([0.3367, 0.1288])\n",
            "\n",
            "\n",
            "===============================================\n",
            "===== RNN (Recurrent Neural Network) =====\n",
            "===============================================\n",
            "\n",
            "[RNN INFO] RNNs process sequences one item at a time, like reading a sentence word by word.\n",
            "           They have a 'hidden state' (memory) that tries to capture what's been seen so far.\n",
            "           Let's see how it processes 'a b c'.\n",
            "\n",
            "[RNN SETUP] Initializing RNN 'weights' and 'bias'.\n",
            "            These are numbers the RNN would learn if we were training it.\n",
            "            - 'input_to_hidden_weights': Transform the current character's vector.\n",
            "            - 'hidden_to_hidden_weights': Transform the previous memory.\n",
            "            - 'hidden_bias': A small adjustment.\n",
            "\n",
            "[RNN SETUP] Initializing the RNN's 'hidden state' (memory) to all zeros.\n",
            "            Before seeing any input, the memory is blank: tensor([0., 0.])\n",
            "\n",
            "--- RNN Step 1: Processing Token 'a' ---\n",
            "Input vector (embedding) for 'a': tensor([0.3367, 0.1288])\n",
            "Hidden state (memory from PREVIOUS step): tensor([0., 0.])\n",
            "\n",
            "[RNN CALC] Updating hidden state using the formula:\n",
            "           new_hidden = tanh( (current_input_vector @ W_input) + (previous_hidden_state @ W_hidden) + bias )\n",
            "Updated RNN hidden state after 'a': tensor([ 0.6657, -0.1784])\n",
            "           The 'tanh' function squashes the result between -1 and 1, keeping numbers manageable.\n",
            "\n",
            "--- RNN Step 2: Processing Token 'b' ---\n",
            "Input vector (embedding) for 'b': tensor([0.2345, 0.2303])\n",
            "Hidden state (memory from PREVIOUS step): tensor([ 0.6657, -0.1784])\n",
            "\n",
            "[RNN CALC] Updating hidden state using the formula:\n",
            "           new_hidden = tanh( (current_input_vector @ W_input) + (previous_hidden_state @ W_hidden) + bias )\n",
            "Updated RNN hidden state after 'b': tensor([0.6539, 0.6365])\n",
            "           The 'tanh' function squashes the result between -1 and 1, keeping numbers manageable.\n",
            "\n",
            "--- RNN Step 3: Processing Token 'c' ---\n",
            "Input vector (embedding) for 'c': tensor([-1.1229, -0.1863])\n",
            "Hidden state (memory from PREVIOUS step): tensor([0.6539, 0.6365])\n",
            "\n",
            "[RNN CALC] Updating hidden state using the formula:\n",
            "           new_hidden = tanh( (current_input_vector @ W_input) + (previous_hidden_state @ W_hidden) + bias )\n",
            "Updated RNN hidden state after 'c': tensor([-0.9068,  0.1196])\n",
            "           The 'tanh' function squashes the result between -1 and 1, keeping numbers manageable.\n",
            "\n",
            "[RNN RESULT] Final RNN hidden state after processing 'a b c': tensor([-0.9068,  0.1196])\n",
            "             This vector is the RNN's attempt to summarize the entire sequence 'a b c'.\n",
            "             A challenge for RNNs: they can struggle to remember things from far back in long sequences.\n",
            "\n",
            "\n",
            "=====================================================\n",
            "===== LSTM (Long Short-Term Memory) =====\n",
            "=====================================================\n",
            "\n",
            "[LSTM INFO] LSTMs are a type of RNN, but with a more sophisticated memory system.\n",
            "            They use 'gates' to control the flow of information, helping them remember things for longer.\n",
            "            The key components are:\n",
            "            - A 'cell state' (C_t): The long-term memory conveyor belt.\n",
            "            - Forget Gate (f_t): Decides what old information to discard from C_t.\n",
            "            - Input Gate (i_t): Decides what new information to store in C_t.\n",
            "            - Output Gate (o_t): Decides what part of C_t to output as the current hidden state (h_t).\n",
            "\n",
            "[LSTM INFO] IMPORTANT: In this demo, the LSTM's weights are random, so it doesn't 'know' what to forget yet.\n",
            "             It 'learns' what's important to forget/remember during a separate 'training' phase, not shown here.\n",
            "             We're just seeing the *mechanics* of how the gates operate with some random weights.\n",
            "\n",
            "[LSTM SETUP] Initializing LSTM 'weights' and 'bias'. LSTMs have more weights because of their gates.\n",
            "             These weights combine the current input and previous hidden state to control 4 parts.\n",
            "\n",
            "[LSTM SETUP] Initializing LSTM 'hidden state' (h_t, short-term output) and 'cell state' (C_t, long-term memory) to zeros.\n",
            "            Initial hidden state (h_0): tensor([0., 0.])\n",
            "            Initial cell state (C_0): tensor([0., 0.])\n",
            "\n",
            "--- LSTM Step 1: Processing Token 'a' ---\n",
            "Input vector (embedding for 'a'): tensor([0.3367, 0.1288])\n",
            "Hidden state from PREVIOUS step (h_{0}): tensor([0., 0.])\n",
            "Cell state (long-term memory) from PREVIOUS step (C_{0}): tensor([0., 0.])\n",
            "\n",
            "[LSTM CALC] Combining current input vector and previous hidden state (h_t-1).\n",
            "[LSTM CALC] Calculating values for all 4 gates/parts using the combined vector and LSTM weights.\n",
            "\n",
            "[LSTM CALC] Applying activation functions to gate values:\n",
            "            - 'sigmoid' (outputs 0 to 1) is used for gates. This is key!\n",
            "              A value near 0 means 'close the gate / forget / ignore'.\n",
            "              A value near 1 means 'open the gate / remember / allow'.\n",
            "            - 'tanh' (outputs -1 to 1) is used for new candidate values to add to memory.\n",
            "\n",
            "Forget Gate (f_t) values: tensor([0.2918, 0.4924])\n",
            "           Interpretation: Each number (0 to 1) decides how much of the corresponding\n",
            "           part of the *old* cell state (C_{0}) to keep. 0 = forget, 1 = keep.\n",
            "\n",
            "Input Gate (i_t) values: tensor([0.3893, 0.5057])\n",
            "           Interpretation: Decides how much of the 'new candidate info' (below) to add to memory.\n",
            "Cell Candidate (g_t) values (new potential info): tensor([-0.5829, -0.1636])\n",
            "           Interpretation: These are new values that *could* be added to the cell state.\n",
            "\n",
            "Output Gate (o_t) values: tensor([0.4530, 0.6226])\n",
            "           Interpretation: Decides how much of the *newly updated* cell state (C_t) to output as the hidden state (h_t).\n",
            "\n",
            "[LSTM CALC] Updating the cell state (C_t) - the LSTM's long-term memory:\n",
            "           Formula: C_{1} = (Forget_Gate * C_{0}) + (Input_Gate * Cell_Candidate)\n",
            "           Breaking it down:\n",
            "             1. `Forget_Gate * C_{0}`: tensor([0., 0.]) (Old memory filtered by forget gate)\n",
            "             2. `Input_Gate * Cell_Candidate`: tensor([-0.2269, -0.0827]) (New candidate info filtered by input gate)\n",
            "Updated cell state (C_{1}): tensor([-0.2269, -0.0827])\n",
            "           This new cell state C_{1} now holds a combination of filtered old memories and filtered new information.\n",
            "\n",
            "[LSTM CALC] Updating the hidden state (h_t) - the output for this step:\n",
            "           Formula: h_{1} = Output_Gate * tanh(C_{1})\n",
            "Updated hidden state (h_{1}): tensor([-0.1011, -0.0514])\n",
            "           The hidden state h_{1} is a filtered version of the LSTM's internal long-term memory (C_{1}).\n",
            "\n",
            "--- LSTM Step 2: Processing Token 'b' ---\n",
            "Input vector (embedding for 'b'): tensor([0.2345, 0.2303])\n",
            "Hidden state from PREVIOUS step (h_{1}): tensor([-0.1011, -0.0514])\n",
            "Cell state (long-term memory) from PREVIOUS step (C_{1}): tensor([-0.2269, -0.0827])\n",
            "\n",
            "[LSTM CALC] Combining current input vector and previous hidden state (h_t-1).\n",
            "[LSTM CALC] Calculating values for all 4 gates/parts using the combined vector and LSTM weights.\n",
            "\n",
            "[LSTM CALC] Applying activation functions to gate values:\n",
            "            - 'sigmoid' (outputs 0 to 1) is used for gates. This is key!\n",
            "              A value near 0 means 'close the gate / forget / ignore'.\n",
            "              A value near 1 means 'open the gate / remember / allow'.\n",
            "            - 'tanh' (outputs -1 to 1) is used for new candidate values to add to memory.\n",
            "\n",
            "Forget Gate (f_t) values: tensor([0.3652, 0.5318])\n",
            "           Interpretation: Each number (0 to 1) decides how much of the corresponding\n",
            "           part of the *old* cell state (C_{1}) to keep. 0 = forget, 1 = keep.\n",
            "\n",
            "Input Gate (i_t) values: tensor([0.3566, 0.4900])\n",
            "           Interpretation: Decides how much of the 'new candidate info' (below) to add to memory.\n",
            "Cell Candidate (g_t) values (new potential info): tensor([-0.4769,  0.0895])\n",
            "           Interpretation: These are new values that *could* be added to the cell state.\n",
            "\n",
            "Output Gate (o_t) values: tensor([0.4355, 0.5440])\n",
            "           Interpretation: Decides how much of the *newly updated* cell state (C_t) to output as the hidden state (h_t).\n",
            "\n",
            "[LSTM CALC] Updating the cell state (C_t) - the LSTM's long-term memory:\n",
            "           Formula: C_{2} = (Forget_Gate * C_{1}) + (Input_Gate * Cell_Candidate)\n",
            "           Breaking it down:\n",
            "             1. `Forget_Gate * C_{1}`: tensor([-0.0829, -0.0440]) (Old memory filtered by forget gate)\n",
            "             2. `Input_Gate * Cell_Candidate`: tensor([-0.1701,  0.0438]) (New candidate info filtered by input gate)\n",
            "Updated cell state (C_{2}): tensor([-2.5294e-01, -1.5543e-04])\n",
            "           This new cell state C_{2} now holds a combination of filtered old memories and filtered new information.\n",
            "\n",
            "[LSTM CALC] Updating the hidden state (h_t) - the output for this step:\n",
            "           Formula: h_{2} = Output_Gate * tanh(C_{2})\n",
            "Updated hidden state (h_{2}): tensor([-1.0785e-01, -8.4547e-05])\n",
            "           The hidden state h_{2} is a filtered version of the LSTM's internal long-term memory (C_{2}).\n",
            "\n",
            "--- LSTM Step 3: Processing Token 'c' ---\n",
            "Input vector (embedding for 'c'): tensor([-1.1229, -0.1863])\n",
            "Hidden state from PREVIOUS step (h_{2}): tensor([-1.0785e-01, -8.4547e-05])\n",
            "Cell state (long-term memory) from PREVIOUS step (C_{2}): tensor([-2.5294e-01, -1.5543e-04])\n",
            "\n",
            "[LSTM CALC] Combining current input vector and previous hidden state (h_t-1).\n",
            "[LSTM CALC] Calculating values for all 4 gates/parts using the combined vector and LSTM weights.\n",
            "\n",
            "[LSTM CALC] Applying activation functions to gate values:\n",
            "            - 'sigmoid' (outputs 0 to 1) is used for gates. This is key!\n",
            "              A value near 0 means 'close the gate / forget / ignore'.\n",
            "              A value near 1 means 'open the gate / remember / allow'.\n",
            "            - 'tanh' (outputs -1 to 1) is used for new candidate values to add to memory.\n",
            "\n",
            "Forget Gate (f_t) values: tensor([0.9465, 0.5548])\n",
            "           Interpretation: Each number (0 to 1) decides how much of the corresponding\n",
            "           part of the *old* cell state (C_{2}) to keep. 0 = forget, 1 = keep.\n",
            "\n",
            "Input Gate (i_t) values: tensor([0.7714, 0.4588])\n",
            "           Interpretation: Decides how much of the 'new candidate info' (below) to add to memory.\n",
            "Cell Candidate (g_t) values (new potential info): tensor([0.9598, 0.6982])\n",
            "           Interpretation: These are new values that *could* be added to the cell state.\n",
            "\n",
            "Output Gate (o_t) values: tensor([0.6002, 0.1332])\n",
            "           Interpretation: Decides how much of the *newly updated* cell state (C_t) to output as the hidden state (h_t).\n",
            "\n",
            "[LSTM CALC] Updating the cell state (C_t) - the LSTM's long-term memory:\n",
            "           Formula: C_{3} = (Forget_Gate * C_{2}) + (Input_Gate * Cell_Candidate)\n",
            "           Breaking it down:\n",
            "             1. `Forget_Gate * C_{2}`: tensor([-2.3942e-01, -8.6236e-05]) (Old memory filtered by forget gate)\n",
            "             2. `Input_Gate * Cell_Candidate`: tensor([0.7404, 0.3203]) (New candidate info filtered by input gate)\n",
            "Updated cell state (C_{3}): tensor([0.5010, 0.3202])\n",
            "           This new cell state C_{3} now holds a combination of filtered old memories and filtered new information.\n",
            "\n",
            "[LSTM CALC] Updating the hidden state (h_t) - the output for this step:\n",
            "           Formula: h_{3} = Output_Gate * tanh(C_{3})\n",
            "Updated hidden state (h_{3}): tensor([0.2778, 0.0413])\n",
            "           The hidden state h_{3} is a filtered version of the LSTM's internal long-term memory (C_{3}).\n",
            "\n",
            "[LSTM RESULT] Final LSTM hidden state (h_N) after processing 'a b c': tensor([0.2778, 0.0413])\n",
            "[LSTM RESULT] Final LSTM cell state (C_N) after processing 'a b c': tensor([0.5010, 0.3202])\n",
            "              LSTMs, through their gates (especially the forget gate), can learn what information\n",
            "              to retain or discard over long sequences. This 'learning' happens during training,\n",
            "              where weights are adjusted to minimize errors on a specific task.\n",
            "\n",
            "\n",
            "===============================================\n",
            "===== Transformer Self-Attention =====\n",
            "===============================================\n",
            "\n",
            "[TRANSFORMER INFO] Transformers (specifically, their 'self-attention' mechanism) work differently.\n",
            "                   Instead of step-by-step, they look at ALL tokens in the sequence AT ONCE.\n",
            "                   Self-attention helps each token figure out how relevant other tokens are to it.\n",
            "                   Analogy: When reading, you might glance at other words in a sentence to understand a specific word's context.\n",
            "\n",
            "[TRANSFORMER SETUP] First, get the embedding vectors for our entire input sequence 'a b c' at once.\n",
            "Input vectors (embeddings for 'a', 'b', 'c'):\n",
            " tensor([[ 0.3367,  0.1288],\n",
            "        [ 0.2345,  0.2303],\n",
            "        [-1.1229, -0.1863]])\n",
            "\n",
            "[TRANSFORMER SETUP] For each input token, we create three special versions using learned weights:\n",
            "                   1. Query (Q): What is this token 'looking for' in other tokens?\n",
            "                   2. Key (K): What kind of information does this token 'represent' or 'offer'?\n",
            "                   3. Value (V): What actual information does this token 'contribute' if it's deemed relevant?\n",
            "\n",
            "[TRANSFORMER CALC] Generating Query, Key, and Value vectors for each token:\n",
            "Query vectors (Q) - one row per token ('a','b','c'):\n",
            " tensor([[ 0.0143,  0.3568],\n",
            "        [ 0.0287,  0.3699],\n",
            "        [-0.0153, -0.9796]])\n",
            "Key vectors (K) - one row per token:\n",
            " tensor([[-0.3227, -0.3261],\n",
            "        [-0.2036, -0.2565],\n",
            "        [ 1.1125,  1.0369]])\n",
            "Value vectors (V) - one row per token:\n",
            " tensor([[-0.0700,  0.3073],\n",
            "        [ 0.0170,  0.1852],\n",
            "        [ 0.3474, -1.0748]])\n",
            "\n",
            "[TRANSFORMER CALC] Calculating 'Attention Scores':\n",
            "                   How much should token_i 'pay attention' to token_j?\n",
            "                   This is done by: (Query_i @ Key_j.transposed) / sqrt(dimension_of_key)\n",
            "Attention scores (raw, after scaling):\n",
            " tensor([[-0.0855, -0.0668,  0.2728],\n",
            "        [-0.0919, -0.0712,  0.2938],\n",
            "        [ 0.2294,  0.1799, -0.7302]])\n",
            "Interpretation: attention_scores[row_i, col_j] = score of token_i attending to token_j.\n",
            "e.g., scores[0,1] is 'a' attending to 'b'. Scaling helps stabilize numbers.\n",
            "\n",
            "[TRANSFORMER CALC] Converting scores to 'Attention Weights' (probabilities) using Softmax.\n",
            "                   Softmax makes sure that for each token, its attention to all other tokens sums up to 1 (like percentages).\n",
            "Attention weights (after softmax):\n",
            " tensor([[0.2899, 0.2954, 0.4148],\n",
            "        [0.2864, 0.2924, 0.4212],\n",
            "        [0.4283, 0.4076, 0.1641]])\n",
            "Interpretation: weights[row_i, col_j] = how much % token_i focuses on token_j.\n",
            "\n",
            "[TRANSFORMER CALC] Creating new, 'context-aware' representations for each token.\n",
            "                   Each token's new vector = weighted sum of ALL Value vectors.\n",
            "                   The weights are the attention_weights we just calculated.\n",
            "\n",
            "Attention output vectors (new representations for 'a','b','c' based on context):\n",
            " tensor([[ 0.1288, -0.3020],\n",
            "        [ 0.1312, -0.3105],\n",
            "        [ 0.0339,  0.0308]])\n",
            "Each row is a new vector for 'a', 'b', or 'c', now enriched with context from other tokens it 'attended' to.\n",
            "Unlike RNN/LSTM's single final state, Attention gives an updated representation for *each* token.\n",
            "\n",
            "\n",
            "==========================\n",
            "===== Quick Summary =====\n",
            "==========================\n",
            "\n",
            "--- RNN (Recurrent Neural Network) ---\n",
            "- Reads input one piece at a time (sequential).\n",
            "- Uses a 'hidden state' as its memory, updated at each step.\n",
            "- Analogy: Reading a book word-by-word, trying to remember the plot.\n",
            "- Challenge: Can struggle with long-term memory (forgetting early parts of long sequences).\n",
            "\n",
            "--- LSTM (Long Short-Term Memory) ---\n",
            "- Also sequential, but with a smarter memory system using 'gates' (input, forget, output).\n",
            "- Has a 'cell state' for better long-term memory retention. The 'forget gate' specifically learns\n",
            "  what old information in the cell state is no longer relevant and should be discarded.\n",
            "- Analogy: Reading with a good note-taking system – deciding what to write down, what to erase (forget gate's job!), and what to refer to.\n",
            "- Advantage: Better at handling long sequences and remembering important details over time than basic RNNs because it can learn to manage its memory.\n",
            "\n",
            "--- Transformer (Self-Attention) ---\n",
            "- Looks at ALL input tokens at once (parallel processing).\n",
            "- 'Self-attention' lets each token weigh the importance of all other tokens (including itself) to understand its context.\n",
            "- Uses Query, Key, Value mechanism to calculate these attention weights.\n",
            "- Analogy: Looking at an entire picture at once, and for each object, seeing how it relates to all other objects in the scene.\n",
            "- Advantages: Often better for long sequences, can capture complex relationships, good for parallel computation (training faster).\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "--- End of Demo! ---\n",
            "----------------------------------------------------------------------\n",
            "This was a tiny peek. Real models are much bigger and are 'trained' on lots of data to learn their weights,\n",
            "including how the LSTM's forget gate should behave for specific tasks!\n"
          ]
        }
      ]
    }
  ]
}